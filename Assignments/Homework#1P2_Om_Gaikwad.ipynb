{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearRegression, Ridge, Lasso\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PolynomialFeatures\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Parameters\n",
    "best_m = 3              # Optimal polynomial degree\n",
    "number_of_samples = 50  # Number of data points\n",
    "noise_scale = 100       # Noise level\n",
    "\n",
    "# Generate synthetic dataset\n",
    "x = 30 * (np.random.rand(number_of_samples, 1) - 0.5)\n",
    "y = 6 * x + 7 * x**2 + 3 * x**3 + noise_scale * np.random.randn(number_of_samples, 1)\n",
    "\n",
    "# Polynomial feature transformation\n",
    "poly = PolynomialFeatures(degree=best_m)\n",
    "x_poly = poly.fit_transform(x)\n",
    "\n",
    "# Define regression models\n",
    "models = {\n",
    "    \"Unregularized (Linear Regression)\": LinearRegression(),\n",
    "    \"Ridge Regression (L2)\": Ridge(alpha=1.0),\n",
    "    \"Lasso Regression (L1)\": Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "# Train models and calculate MSE\n",
    "mse_results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(x_poly, y)\n",
    "    mse = mean_squared_error(y, model.predict(x_poly))\n",
    "    mse_results[name] = mse\n",
    "\n",
    "# Display MSE values\n",
    "for name, mse in mse_results.items():\n",
    "    print(f\"MSE ({name}): {mse:.2f}\")\n",
    "\n",
    "# Generate predictions for visualization\n",
    "x_range = np.linspace(min(x), max(x), 100).reshape(-1, 1)\n",
    "x_poly_range = poly.transform(x_range)\n",
    "\n",
    "predictions = {name: model.predict(x_poly_range) for name, model in models.items()}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y, color='red', label=\"Noisy Data\")\n",
    "\n",
    "# Plot each model's predictions\n",
    "for name, y_pred in predictions.items():\n",
    "    plt.plot(x_range, y_pred, label=name)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Polynomial Regression (Degree {best_m}) with Regularization')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Analysis\n",
    "analysis = \"\"\"\n",
    "The MSE values for the unregularized, Ridge, and Lasso models are very similar. This is expected because the data is generated from a cubic function, which matches the chosen polynomial degree (3). Therefore, regularization doesn't significantly impact the model performance.\n",
    "\n",
    "Regularization techniques like Ridge (L2) and Lasso (L1) are more beneficial when dealing with higher-order polynomials prone to overfitting. Ridge helps stabilize predictions by shrinking coefficients, while Lasso can reduce complexity by eliminating less important features.\n",
    "\"\"\"\n",
    "\n",
    "print(analysis)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
